{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data preprocessing & feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import warnings\n",
    "\n",
    "register_matplotlib_converters()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 1000, 'display.max_rows', 1000, 'display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "health = pd.read_csv(\"../input/dataset/Health.csv\")\n",
    "holidays = pd.read_csv(\"../input/dataset/Holidays.csv\")\n",
    "data = pd.read_csv(\"../input/dataset/SelectedData.2.1.csv\")\n",
    "products = pd.read_csv(\"../input/dataset/SelectedProducts.2.1.csv\")\n",
    "samples = pd.read_csv(\"../input/dataset/SelectedSamples.2.1.csv\")\n",
    "trends = pd.read_csv(\"../input/dataset/Trends.csv\")\n",
    "weather = pd.read_csv(\"../input/dataset/WeatherFeatures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Merge datasets, create time series stock-keeping unit and add quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge products and data to be able to extract Quantity of product groups\n",
    "data = data.drop_duplicates(subset=('Pharmacy', 'Product', 'Brand', 'Strength'), keep='first')\n",
    "samples_merge = pd.merge(products[['Pharmacy', 'Product', 'Brand', 'Strength']],\n",
    "                         data[['Pharmacy', 'Product', 'Brand', 'Strength', 'Category', 'Description', 'Country',\n",
    "                               'City']], on=['Pharmacy', 'Product', 'Brand', 'Strength'])\n",
    "# Extract product group\n",
    "dataframe = samples_merge[(samples_merge['Category'].str.contains('malaria', case=False)) |\n",
    "                          (samples_merge['Description'].str.contains('malaria', case=False))]\n",
    "# Add weather features to get time series for every stock keeping unit\n",
    "dataframe = pd.merge(dataframe, weather, on=('City', 'Country'), how='left')\n",
    "# Add Quantity\n",
    "dataframe = pd.merge(dataframe, samples, on=(['Pharmacy', 'Product', 'Brand', 'Strength', 'Year', 'Week']), how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Replace quantity of 1/2018 if the three quantities before and after are NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quantity and frequency of time series with 2018-01 > 0 but the three quantities before and after = 0\n",
    "m = (dataframe['Quantity'].notna()\n",
    "     .groupby(dataframe['Pharmacy'])\n",
    "     .rolling(7, center=True).sum().le(1)\n",
    "     .reset_index(level='Pharmacy', drop='Pharmacy'))\n",
    "dataframe['Quantity'] = dataframe['Quantity'].mask(m & dataframe['Year'].eq(2018) & dataframe['Week'].eq(1))\n",
    "n = (dataframe['Frequency'].notna()\n",
    "     .groupby(dataframe['Pharmacy'])\n",
    "     .rolling(7, center=True).sum().le(1)\n",
    "     .reset_index(level='Pharmacy', drop='Pharmacy'))\n",
    "dataframe['Frequency'] = dataframe['Frequency'].mask(n & dataframe['Year'].eq(2018) & dataframe['Week'].eq(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Drop before first and after last occurrence for every time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[['Pharmacy', 'Product', 'Brand', 'Strength']] \\\n",
    "    = dataframe[['Pharmacy', 'Product', 'Brand', 'Strength']].fillna('missing')  # otherwise NaNs are dropped\n",
    "dataframe = dataframe.groupby(['Pharmacy', 'Product', 'Brand', 'Strength'], group_keys=False) \\\n",
    "    .apply(lambda x: x.loc[x['Quantity'].first_valid_index():x['Quantity'].last_valid_index()])\n",
    "# replace NaN by 0 in column Quantity\n",
    "dataframe['Quantity'].fillna(0, inplace=True)\n",
    "dataframe['Quantity'] = dataframe['Quantity'].astype('int32')  # Quantity to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Data cleaning: Feature *Price* (See data-analysis.ipynb for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean prices (See data-analysis.ipynb for more information)\n",
    "data = data.drop_duplicates(subset=('Pharmacy', 'Product', 'Brand', 'Strength'), keep='first')\n",
    "data.loc[data['Price'] > 60000, 'Price'] = data['Cost'] * 1.2  # replace outlier with cost * 1.2\n",
    "data.loc[data['Price'] == 0, 'Price'] = data['Cost'] * 1.2  # replace price 0.00 with cost * 1.2\n",
    "# Add Price\n",
    "dataframe = pd.merge(dataframe, data[['Pharmacy', 'Product', 'Brand', 'Strength', 'Price']],\n",
    "                     on=(['Pharmacy', 'Product', 'Brand', 'Strength']), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Descriptive statistics for *Quantity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    123701.000000\n",
       "mean         20.039975\n",
       "std         147.456589\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           3.000000\n",
       "max        9012.000000\n",
       "Name: Quantity, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['Quantity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Columns with missing values initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Description',\n",
       " 'City',\n",
       " 'Unnamed: 0_x',\n",
       " 'Year',\n",
       " 'Week',\n",
       " 'PrecipInt',\n",
       " 'PrecipProb',\n",
       " 'Humidity',\n",
       " 'WindSpeed',\n",
       " 'CloudCover',\n",
       " 'UVIndex',\n",
       " 'TempMin',\n",
       " 'TempMax',\n",
       " 'PrecipInt_1',\n",
       " 'PrecipInt.avg_2',\n",
       " 'PrecipInt.avg_5',\n",
       " 'PrecipInt.avg.Lag1_5',\n",
       " 'PrecipInt.5W.avg.Lag2',\n",
       " 'PrecipInt.5W.avg.Lag3',\n",
       " 'PrecipInt.6W.avg.Lag4',\n",
       " 'PrecipInt.LookAhead_1',\n",
       " 'PrecipInt.LookAhead_2',\n",
       " 'PrecipProb_1',\n",
       " 'PrecipProb.avg_2',\n",
       " 'PrecipProb.avg_5',\n",
       " 'PrecipProb.avg.Lag1_5',\n",
       " 'PrecipProb.5W.avg.Lag2',\n",
       " 'PrecipProb.5W.avg.Lag3',\n",
       " 'PrecipProb.6W.avg.Lag4',\n",
       " 'PrecipProb.LookAhead_1',\n",
       " 'PrecipProb.LookAhead_2',\n",
       " 'Humidity_1',\n",
       " 'Humidity.avg_2',\n",
       " 'Humidity.avg_5',\n",
       " 'Humidity.avg.Lag1_5',\n",
       " 'Humidity.avg.Lag2_5',\n",
       " 'Humidity.avg.Lag3_5',\n",
       " 'Humidity.avg.Lag4_6',\n",
       " 'Humidity.LookAhead_1',\n",
       " 'Humidity.LookAhead_2',\n",
       " 'WindSpeed_1',\n",
       " 'WindSpeed.avg_2',\n",
       " 'WindSpeed.avg_5',\n",
       " 'WindSpeed.avg.Lag1_5',\n",
       " 'WindSpeed.avg.Lag2_5',\n",
       " 'WindSpeed.avg.Lag3_5',\n",
       " 'WindSpeed.avg.Lag4_6',\n",
       " 'WindSpeed.LookAhead_1',\n",
       " 'WindSpeed.LookAhead_2',\n",
       " 'CloudCover_1',\n",
       " 'CloudCover.avg_2',\n",
       " 'CloudCover.avg_5',\n",
       " 'CloudCover.avg.Lag1_5',\n",
       " 'CloudCover.avg.Lag2_5',\n",
       " 'CloudCover.avg.Lag3_5',\n",
       " 'CloudCover.avg.Lag4_6',\n",
       " 'CloudCover.LookAhead_1',\n",
       " 'CloudCover.LookAhead_2',\n",
       " 'UVIndex_1',\n",
       " 'UVIndex.avg_2',\n",
       " 'UVIndex.avg_5',\n",
       " 'UVIndex.avg.Lag1_5',\n",
       " 'UVIndex.avg.Lag2_5',\n",
       " 'UVIndex.avg.Lag3_5',\n",
       " 'UVIndex.avg.Lag4_6',\n",
       " 'UVIndex.LookAhead_1',\n",
       " 'UVIndex.LookAhead_2',\n",
       " 'TempMin_1',\n",
       " 'TempMin.avg_2',\n",
       " 'TempMin.avg_5',\n",
       " 'TempMin.avg.Lag1_5',\n",
       " 'TempMin.avg.Lag2_5',\n",
       " 'TempMin.avg.Lag3_5',\n",
       " 'TempMin.avg.Lag4_6',\n",
       " 'TempMin.LookAhead_1',\n",
       " 'TempMin.LookAhead_2',\n",
       " 'TempMax_1',\n",
       " 'TempMax.avg_2',\n",
       " 'TempMax.avg_5',\n",
       " 'TempMax.avg.Lag1_5',\n",
       " 'TempMax.avg.Lag2_5',\n",
       " 'TempMax.avg.Lag3_5',\n",
       " 'TempMax.avg.Lag4_6',\n",
       " 'TempMax.LookAhead_1',\n",
       " 'TempMax.LookAhead_2',\n",
       " 'Unnamed: 0_y',\n",
       " 'Q.min',\n",
       " 'F.min',\n",
       " 'Q.mean',\n",
       " 'F.mean',\n",
       " 'Q.median',\n",
       " 'F.median',\n",
       " 'Q.max',\n",
       " 'F.max',\n",
       " 'Q.var',\n",
       " 'F.var',\n",
       " 'Frequency',\n",
       " 'Price']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns[dataframe.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Standardization of same Products/Brands/Strengths with different spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "porterStemmer = PorterStemmer()  # function for Stemming\n",
    "\n",
    "\n",
    "def stem_strings(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmedTokens = [porterStemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmedTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "a) Feature *Product*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique products has been reduced from 1152 to 394 (66 % reduction).\n"
     ]
    }
   ],
   "source": [
    "# Product: Standardization of different spellings\n",
    "uniqueProductBefore = dataframe['Product'].nunique()\n",
    "\n",
    "dataframe['Product'] = dataframe['Product'].str.lower().str.replace(\"[:;.,+'-'_/%()']\", \" \").str.replace('\\\\', ' ') \\\n",
    "    .str.replace('[0123456789]', '').str.replace('[\\s+[a-zA-Z]\\s+]', '').str.replace('mg', '')\n",
    "dataframe['Product'] = dataframe['Product'].apply(stem_strings)\n",
    "\n",
    "dataframe['Product'] = dataframe['Product'].str.findall('\\w{4,}').str.join(' ')  # remove words <4 characters\n",
    "dataframe['Product'] = dataframe['Product'].str.replace('tablet|tab|syrup|syrp', ' ').str.replace('  ', ' ').str.strip()\n",
    "dataframe['Product'].replace(to_replace=[r'[l]?a?r?te[r]?[m]?[e]?t[h]?', r'lum[aei]f[ae]n[dt]?[h]?[r]?i[mn]',\n",
    "                                         r'p[io]per[a]?quin', r'sulph[au]methopyrazin[e]',\n",
    "                                         r'd[iy]h[y]?droa[r]?tem[ei]sin[i]?[n]?'],\n",
    "                             value=['artemether', 'lumefantrin', 'piperaquin', 'sulphamethopyrazin',\n",
    "                                    'dihydroartemisinin'],\n",
    "                             regex=True, inplace=True)\n",
    "dataframe['Product'] = dataframe['Product'].str.replace('  ', ' ')\n",
    "\n",
    "# print(pd.DataFrame(dataframe['Product'].unique()))\n",
    "uniqueProductAfter = dataframe['Product'].nunique()\n",
    "print('The number of unique products has been reduced from {0} to {1} ({2:.0f} % reduction).'\n",
    "      .format(uniqueProductBefore, uniqueProductAfter,\n",
    "              ((uniqueProductBefore - uniqueProductAfter) / uniqueProductBefore) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "b) Feature *Brand*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique brands has been reduced from 1319 to 458 (65 % reduction).\n"
     ]
    }
   ],
   "source": [
    "# Brand: Standardization of different spellings\n",
    "uniqueBrandBefore = dataframe['Brand'].nunique()\n",
    "\n",
    "dataframe['Brand'] = dataframe['Brand'].str.lower().str.replace(\"[:;.,+'-'_/%()']\", \" \").str.replace('\\\\', ' ') \\\n",
    "    .str.replace('[0123456789]', '').str.replace('mg|ml', ' ').str.replace('-', ' ').str.replace('ñ', 'n')\n",
    "dataframe['Brand'] = dataframe['Brand'].str.findall('\\w{4,}').str.join(' ')  # remove words shorter than 4 characters\n",
    "dataframe['Brand'] = dataframe['Brand'].str.replace('tablets|tabs|syrup|syrp|susp|above', ' ') \\\n",
    "    .str.replace(\"  \", \" \").str.strip()\n",
    "dataframe['Brand'].replace(to_replace=[r'a?r?te[r]?[m]?[e]?[n]?t[h]?[e]?[r]?'], value=['artemeth'], regex=True,\n",
    "                           inplace=True)\n",
    "dataframe['Brand'].loc[dataframe['Brand'].str.contains('dabur', case=False)\n",
    "                       | dataframe['Brand'].str.contains('odomos', case=False)] = 'dabur odomos'\n",
    "dataframe['Brand'] = dataframe['Brand'].apply(stem_strings)\n",
    "dataframe['Brand'] = dataframe['Brand'].str.replace('  ', ' ')\n",
    "\n",
    "# print(pd.DataFrame(dataframe['Brand'].unique()))\n",
    "uniqueBrandAfter = dataframe['Brand'].nunique()\n",
    "print('The number of unique brands has been reduced from {0} to {1} ({2:.0f} % reduction).'\n",
    "      .format(uniqueBrandBefore, uniqueBrandAfter,\n",
    "              ((uniqueBrandBefore - uniqueBrandAfter) / uniqueBrandBefore) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "c) Feature *Strength*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique strengths has been reduced from 405 to 148 (63 % reduction).\n"
     ]
    }
   ],
   "source": [
    "# Strength: Standardization of different spellings\n",
    "uniqueStrengthBefore = dataframe['Strength'].nunique()\n",
    "\n",
    "dataframe['Strength'] = dataframe['Strength'].str.lower().str.replace(\"[:.;,+'-'_/%()']\", \" \").str.replace('\\\\', ' ') \\\n",
    "    .str.replace('[\\s+[a-zA-Z]\\s+]', '')\n",
    "dataframe['Strength'].replace(\n",
    "    to_replace=[r'[0][ ][m]', r'[1][ ][m]', r'[1][ ][l]', r'[2][ ][m]', r'[5][ ][m]', r'[3][ ][s]',\n",
    "                r'[6][ ][s]', r'[9][ ][s]', r'[12][ ][s]', r'[18][ ][s]', r'[24][ ][s]'],\n",
    "    value=['0m', '1m', '1l', '2m', '5m', '3s', '6s', '9s', '12s', '18s', '24s'], regex=True, inplace=True)\n",
    "dataframe['Strength'] = dataframe['Strength'].str.replace('\\w{7,}', '').str.replace('litr', 'l') \\\n",
    "    .str.replace('mg|ml|gm|g', '')\n",
    "dataframe['Strength'] = dataframe['Strength'].str.replace \\\n",
    "    ('al|ds|artemeth|lumefantrin|usp|bp|per|ph|int|osat|vi|tab|tablet|yellow|x|xx|xxx|actm|i|let',\n",
    "     '').str.strip()\n",
    "dataframe['Strength'] = dataframe['Strength'].apply(stem_strings)\n",
    "dataframe['Strength'] = dataframe['Strength'].str.replace('  ', ' ')\n",
    "\n",
    "# print(pd.DataFrame(dataframe['Strength'].unique()))\n",
    "uniqueStrengthAfter = dataframe['Strength'].nunique()\n",
    "print('The number of unique strengths has been reduced from {0} to {1} ({2:.0f} % reduction).'\n",
    "      .format(uniqueStrengthBefore, uniqueStrengthAfter,\n",
    "              ((uniqueStrengthBefore - uniqueStrengthAfter) / uniqueStrengthBefore) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Group time series that have been standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before grouping standardized time series: 123701\n"
     ]
    }
   ],
   "source": [
    "print('Length before grouping standardized time series:', len(dataframe))\n",
    "dataframe = dataframe.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price', 'Country', 'City', 'Year', 'Week'])[\n",
    "    ['Quantity', 'Frequency']].sum().reset_index()\n",
    "# Drop few sales that are not assigned to product, brand and strength\n",
    "dataframe = dataframe.drop(dataframe.index[(dataframe['Product'] == '') &\n",
    "                                           (dataframe['Brand'] == '') &\n",
    "                                           (dataframe['Strength'] == '')])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "3.1 Dataset-related features"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "a) Medical sales in this pharmacy in previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_ind = dataframe.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price', 'Year', 'Week'])[\n",
    "    'Quantity'].sum().reset_index()\n",
    "\n",
    "Q_ind['Quantity_individual_1'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])['Quantity'].shift(1)\n",
    "Q_ind['Quantity_individual_1'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Quantity_individual_1'].apply(lambda x: x.fillna(x[:11].mean()))  # replace NaN with mean of following 10 weeks\n",
    "Q_ind['Quantity_individual_1'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Quantity_individual_1'].apply(lambda x: x.fillna(0))  # replace remaining NaNs with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "b) Mean sales of this drug in this pharmacy in previous 4 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_ind['Quantity_individual.avg_4'] = Q_ind['Quantity'].rolling(4).mean().shift(1)\n",
    "Q_ind['Quantity_individual.avg_4'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Quantity_individual.avg_4'].apply(lambda x: x.fillna(x[:10].mean()))  # replace NaN with mean of following 5 weeks\n",
    "Q_ind['Quantity_individual.avg_4'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Quantity_individual.avg_4'].apply(lambda x: x.fillna(0))  # replace NaN with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "c) Mean sales of this drug in this pharmacy in previous 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_ind['Quantity_individual.avg_2'] = Q_ind['Quantity'].rolling(2).mean().shift(1)\n",
    "Q_ind['Quantity_individual.avg_2'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Quantity_individual.avg_2'].apply(lambda x: x.fillna(x[:7].mean()))  # replace NaN with mean of following 5 weeks\n",
    "Q_ind['Quantity_individual.avg_2'] = Q_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Quantity_individual.avg_2'].apply(lambda x: x.fillna(0))  # replace NaN with 0\n",
    "Q_ind = Q_ind.drop(['Quantity'], axis=1)\n",
    "dataframe = pd.merge(dataframe, Q_ind, on=('Pharmacy', 'Product', 'Brand', 'Strength', 'Price', 'Year', 'Week'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "d) Total medicals sales for respective product group in previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_total = dataframe.groupby(['Year', 'Week'])['Quantity'].sum().reset_index()\n",
    "Q_total['Quantity_total_1'] = Q_total['Quantity'].shift(1)\n",
    "Q_total = Q_total.drop(['Quantity'], axis=1)\n",
    "Q_total = Q_total.fillna(Q_total[:11].mean())  # replace NaN with mean of the following 10 weeks\n",
    "dataframe = pd.merge(dataframe, Q_total, on=('Year', 'Week'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "e) Total medical sales for respective product group in this pharmacy in previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_pharma = dataframe.groupby(['Pharmacy', 'Year', 'Week'])['Quantity'].sum().reset_index()\n",
    "Q_pharma['Quantity_pharmacy_1'] = Q_pharma.groupby('Pharmacy')['Quantity'].shift(1)\n",
    "Q_pharma['Quantity_pharmacy_1'] = Q_pharma.groupby(['Pharmacy'])['Quantity_pharmacy_1'] \\\n",
    "    .apply(lambda x: x.fillna(x[:11].mean()))  # replace NaN with mean of 10 following weeks\n",
    "Q_pharma['Quantity_pharmacy_1'] = Q_pharma.groupby(['Pharmacy'])['Quantity'] \\\n",
    "    .apply(lambda x: x.fillna(x.mean()))  # replace remaining Nan with overall mean (Pharmacies with less than 11 weeks)\n",
    "Q_pharma = Q_pharma.drop(['Quantity'], axis=1)\n",
    "dataframe = pd.merge(dataframe, Q_pharma, on=('Pharmacy', 'Year', 'Week'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "f) Number of pharmacies with medical sales in previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pharmacy_1 = samples.groupby(['Year', 'Week'])['Pharmacy'].nunique().reset_index()\n",
    "Pharmacy_1['Pharmacy_1'] = Pharmacy_1['Pharmacy'].shift(1)\n",
    "Pharmacy_1['Pharmacy_1'] = Pharmacy_1['Pharmacy_1'].fillna(method='bfill')  # fill Year 2017 Week 1 with value of Week 2\n",
    "Pharmacy_1 = Pharmacy_1.drop(['Pharmacy'], axis=1)\n",
    "dataframe = pd.merge(dataframe, Pharmacy_1, on=('Year', 'Week'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "g) Frequency of this medical sold in this pharmacy in previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_ind = dataframe.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price', 'Year', 'Week'])['Frequency'].sum().reset_index()\n",
    "F_ind['Frequency_individual_1'] = F_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])['Frequency'].shift(1)\n",
    "F_ind['Frequency_individual_1'] = F_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Frequency_individual_1'].apply(lambda x: x.fillna(x[:11].mean()))  # replace NaN with mean of following 10 weeks\n",
    "F_ind['Frequency_individual_1'] = F_ind.groupby(['Pharmacy', 'Product', 'Brand', 'Strength', 'Price'])[\n",
    "    'Frequency_individual_1'].apply(lambda x: x.fillna(0))  # replace remaining NaNs with 0\n",
    "F_ind = F_ind.drop(['Frequency'], axis=1)\n",
    "dataframe = pd.merge(dataframe, F_ind, on=('Pharmacy', 'Product', 'Brand', 'Strength', 'Price', 'Year', 'Week'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Rearrange dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe[['Pharmacy', 'Product', 'Brand', 'Strength', 'Country', 'City', 'Year', 'Week', 'Quantity',\n",
    "                       'Frequency', 'Quantity_individual_1', 'Quantity_individual.avg_4', 'Quantity_individual.avg_2',\n",
    "                       'Quantity_total_1', 'Quantity_pharmacy_1', 'Pharmacy_1', 'Frequency_individual_1', 'Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "3.2 External features"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "a) Weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value for weather features:\n",
      "PrecipInt    -3.457720e-03\n",
      "PrecipProb   -1.690000e-16\n",
      "Humidity      3.357143e-01\n",
      "WindSpeed     1.621429e+00\n",
      "CloudCover    1.660000e-06\n",
      "UVIndex       0.000000e+00\n",
      "TempMin       9.961429e+00\n",
      "TempMax       2.043857e+01\n",
      "dtype: float64\n",
      "PrecipInt_1             -0.003458\n",
      "PrecipInt.avg_2         -0.001729\n",
      "PrecipInt.avg_5         -0.000692\n",
      "PrecipInt.avg.Lag1_5    -0.000692\n",
      "PrecipInt.5W.avg.Lag2   -0.000692\n",
      "PrecipInt.5W.avg.Lag3   -0.000692\n",
      "PrecipInt.6W.avg.Lag4   -0.000576\n",
      "PrecipInt.LookAhead_1   -0.003458\n",
      "PrecipInt.LookAhead_2   -0.001729\n",
      "dtype: float64\n",
      "PrecipProb_1             -1.690000e-16\n",
      "PrecipProb.avg_2         -1.560000e-16\n",
      "PrecipProb.avg_5         -1.160000e-16\n",
      "PrecipProb.avg.Lag1_5    -1.160000e-16\n",
      "PrecipProb.5W.avg.Lag2   -1.160000e-16\n",
      "PrecipProb.5W.avg.Lag3   -1.160000e-16\n",
      "PrecipProb.6W.avg.Lag4   -1.030000e-16\n",
      "PrecipProb.LookAhead_1   -1.690000e-16\n",
      "PrecipProb.LookAhead_2   -1.560000e-16\n",
      "dtype: float64\n",
      "\n",
      "Minimum value for weather features after cleaning:\n",
      "PrecipInt      0.000000\n",
      "PrecipProb     0.000000\n",
      "Humidity       0.335714\n",
      "WindSpeed      1.621429\n",
      "CloudCover     0.000002\n",
      "UVIndex        0.000000\n",
      "TempMin        9.961429\n",
      "TempMax       20.438571\n",
      "dtype: float64\n",
      "PrecipInt_1              0.0\n",
      "PrecipInt.avg_2          0.0\n",
      "PrecipInt.avg_5          0.0\n",
      "PrecipInt.avg.Lag1_5     0.0\n",
      "PrecipInt.5W.avg.Lag2    0.0\n",
      "PrecipInt.5W.avg.Lag3    0.0\n",
      "PrecipInt.6W.avg.Lag4    0.0\n",
      "PrecipInt.LookAhead_1    0.0\n",
      "PrecipInt.LookAhead_2    0.0\n",
      "dtype: float64\n",
      "PrecipProb_1              0.0\n",
      "PrecipProb.avg_2          0.0\n",
      "PrecipProb.avg_5          0.0\n",
      "PrecipProb.avg.Lag1_5     0.0\n",
      "PrecipProb.5W.avg.Lag2    0.0\n",
      "PrecipProb.5W.avg.Lag3    0.0\n",
      "PrecipProb.6W.avg.Lag4    0.0\n",
      "PrecipProb.LookAhead_1    0.0\n",
      "PrecipProb.LookAhead_2    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Minimum value for weather features:')\n",
    "print(weather.loc[:, 'PrecipInt':'TempMax'].min())  # Negative values\n",
    "print(weather.loc[:, 'PrecipInt_1':'PrecipInt.LookAhead_2'].min())  # Negative values\n",
    "print(weather.loc[:, 'PrecipProb_1':'PrecipProb.LookAhead_2'].min())  # Negative value\n",
    "\n",
    "weather.loc[:, 'PrecipInt':'PrecipProb'] = weather.loc[:, 'PrecipInt':'PrecipProb'].apply(\n",
    "    lambda x: np.where(x < 0, 0, x))\n",
    "weather.loc[:, 'PrecipInt_1':'PrecipInt.LookAhead_2'] = weather.loc[:, 'PrecipInt_1':'PrecipInt.LookAhead_2'].apply(\n",
    "    lambda x: np.where(x < 0, 0, x))\n",
    "weather.loc[:, 'PrecipProb_1':'PrecipProb.LookAhead_2'] = weather.loc[:, 'PrecipProb_1':'PrecipProb.LookAhead_2'].apply(\n",
    "    lambda x: np.where(x < 0, 0, x))\n",
    "\n",
    "print('\\nMinimum value for weather features after cleaning:')\n",
    "print(weather.loc[:, 'PrecipInt':'TempMax'].min())  # No negative values\n",
    "print(weather.loc[:, 'PrecipInt_1':'PrecipInt.LookAhead_2'].min())  # No negative values\n",
    "print(weather.loc[:, 'PrecipProb_1':'PrecipProb.LookAhead_2'].min())  # No negative values\n",
    "\n",
    "dataframe = pd.merge(dataframe, weather, on=('City', 'Country', 'Year', 'Week'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "b) Trends features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends features\n",
    "dataframe = pd.merge(dataframe, trends, on=(['Country', 'Year', 'Week']), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "c) Health features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health features\n",
    "dataframe = pd.merge(dataframe, health, on=(['Country', 'Year', 'Week']), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop year 2019 week 25 (no weather features available)\n",
    "dataframe = dataframe.drop(dataframe.index[dataframe['Year'] == 2019] & dataframe.index[dataframe['Week'] == 25])\n",
    "# Drop Pharmacy 309 (Yangon, Nigeria) & Pharmacy 188 (Tanta, Nigeria) (no weather features available)\n",
    "dataframe = dataframe.drop(dataframe.index[dataframe['Pharmacy'] == 'Pharmacy 309'])\n",
    "dataframe = dataframe.drop(dataframe.index[dataframe['Pharmacy'] == 'Pharmacy 188'])\n",
    "# Drop Pharmacy 183 (Vihiga, Nigeria) (no sales and thus NaN values in multiple columns)\n",
    "dataframe = dataframe.drop(dataframe.index[dataframe['Pharmacy'] == 'Pharmacy 183'])\n",
    "# Drop index from merged dataframes\n",
    "dataframe = dataframe.drop(['Unnamed: 0'], axis=1)\n",
    "# For each time series, replace NaN in the last row of Weather LookAhead\n",
    "dataframe.loc[:, 'PrecipInt':'TempMax.LookAhead_2'] = dataframe.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after grouping standardized time series: 99807\n"
     ]
    }
   ],
   "source": [
    "print('Length after grouping standardized time series:', len(dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "d) Feature *Date*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Date\n",
    "dataframe[['Year', 'Week']] = dataframe[['Year', 'Week']].astype('int32')  # change Year and Week to integer\n",
    "dates = dataframe.Year * 100 + dataframe.Week\n",
    "dataframe['Date'] = pd.to_datetime(dates.astype(str) + '0', format='%Y%W%w')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "e) Feature *Holiday*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Holiday\n",
    "holidays = holidays.drop(['HolidayType'], axis=1)\n",
    "holidays['Date'] = pd.to_datetime(holidays['Date'])  # add a column for week number\n",
    "holidays['Week'] = holidays['Date'].dt.strftime('%U').astype(int)\n",
    "dates = holidays.Year * 100 + holidays.Week\n",
    "holidays['Date'] = pd.to_datetime(dates.astype(str) + '0', format='%Y%W%w')  # change the format to datetime\n",
    "holidays = holidays.drop_duplicates(subset=['Country', 'Date'], keep='first')  # in case of two holidays in a week\n",
    "holidays = holidays.drop(['Unnamed: 0', 'Year', 'Week'], axis=1)\n",
    "dataframe = pd.merge(dataframe, holidays, on=(['Country', 'Date']), how='left')\n",
    "# if Holiday = NaN: change to 0, else change to 1\n",
    "dataframe['Holiday'] = dataframe['Holiday'].where(dataframe['Holiday'].isnull(), 1).fillna(0).astype(int)\n",
    "dataframe = dataframe.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns[dataframe.isnull().any()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "3.3 Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "\n",
    "poly1 = pd.DataFrame(poly.fit_transform(dataframe.loc[:, 'PrecipInt':'TempMax']),\n",
    "                     columns=poly.get_feature_names(dataframe.loc[:, 'PrecipInt':'TempMax'].columns))\n",
    "\n",
    "poly2 = pd.DataFrame(poly.fit_transform(dataframe.loc[:, 'Disease':'Influenza']),\n",
    "                     columns=poly.get_feature_names(dataframe.loc[:, 'Disease':'Influenza'].columns))\n",
    "\n",
    "poly3 = pd.DataFrame(poly.fit_transform(dataframe.loc[:, 'Quantity_individual_1':'Frequency_individual_1']),\n",
    "                     columns=poly.get_feature_names(\n",
    "                         dataframe.loc[:, 'Quantity_individual_1':'Frequency_individual_1'].columns))\n",
    "\n",
    "dataframe = pd.concat([dataframe, poly1.loc[:, 'PrecipInt^2':'TempMax^3']], ignore_index=False, axis=1, sort=False)\n",
    "dataframe = pd.concat([dataframe, poly2.loc[:, 'Disease^2':'Influenza^3']], ignore_index=False, axis=1, sort=False)\n",
    "dataframe = pd.concat([dataframe, poly3.loc[:, 'Quantity_individual_1^2':'Frequency_individual_1^3']],\n",
    "                      ignore_index=False, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "3.4 Feature scaling (MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dataframe.loc[:, 'Quantity_individual_1':'Frequency_individual_1^3'] = \\\n",
    "    scaler.fit_transform(dataframe.loc[:, 'Quantity_individual_1':'Frequency_individual_1^3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "3.5 One-hot encoding (*Pharmacy*, *Product*, *Brand*, *Strength*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for pharmacy, product, brand, strength\n",
    "OneHotEncoding = pd.get_dummies(dataframe[['Pharmacy', 'Product', 'Brand', 'Strength']])\n",
    "dataframe = pd.concat([dataframe, OneHotEncoding], ignore_index=False, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Date\n",
    "dataframe[['Year', 'Week']] = dataframe[['Year', 'Week']].astype('int32')  # change Year and Week to integer\n",
    "dates = dataframe.Year * 100 + dataframe.Week\n",
    "dataframe['Date'] = pd.to_datetime(dates.astype(str) + '0', format='%Y%W%w')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Split the dataframe into lumpy and constant time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into dataframeLumpy and dataframeConstant\n",
    "a = dataframe.groupby(['Pharmacy', 'Product', 'Brand', 'Strength'])['Quantity'].describe()['50%'].reset_index()\n",
    "dataframe = pd.merge(dataframe, a, on=['Pharmacy', 'Product', 'Brand', 'Strength'])\n",
    "dataframe['50%'].values[dataframe['50%'].values > 0] = 1\n",
    "\n",
    "dataframeLumpy = dataframe[dataframe['50%'] == 0]\n",
    "dataframeLumpy = dataframeLumpy.reindex()\n",
    "dataframeLumpy.set_index(['Date'], inplace=True)  # set index to Date\n",
    "\n",
    "dataframeConstant = dataframe[(dataframe['50%'] == 1)]\n",
    "dataframeConstant = dataframeConstant.reindex()\n",
    "dataframeConstant.set_index(['Date'], inplace=True)  # set index to Date\n",
    "\n",
    "dataframe = dataframe.reindex()\n",
    "dataframe.set_index(['Date'], inplace=True)  # set index to Date"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size dataframe total: 99807\n",
      "Size dataframe lumpy: 63423 (64 % of the total data)\n",
      "Size dataframe constant: 36384 (36 % of the total data)\n"
     ]
    }
   ],
   "source": [
    "# Size dataframe\n",
    "print('Size dataframe total: {}'.format(len(dataframe)))\n",
    "print('Size dataframe lumpy: {0} ({1} % of the total data)'.format(\n",
    "    len(dataframeLumpy), round(len(dataframeLumpy) / len(dataframe) * 100)))\n",
    "print('Size dataframe constant: {0} ({1} % of the total data)'.format(\n",
    "    len(dataframeConstant), round(len(dataframeConstant) / len(dataframe) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset-related features:\n",
      "Quantity/Frequency/Pharmacy/Price: 120\n",
      "Pharmacy (one-hot encoding): 231\n",
      "Product (one-hot encoding): 279\n",
      "Brand (one-hot encoding): 352\n",
      "Strength (one-hot encoding): 144\n",
      "\n",
      "External features:\n",
      "Weather: 236\n",
      "Trends: 70\n",
      "Health/Holiday: 4\n",
      "\n",
      "Total: 1436\n"
     ]
    }
   ],
   "source": [
    "# Number of features\n",
    "print('Dataset-related features:')\n",
    "print('Quantity/Frequency/Pharmacy/Price:', len(dataframe.filter(regex='Quantity_|Frequency_|Pharmacy_1|Price') .columns))\n",
    "print('Pharmacy (one-hot encoding):', len(dataframe.filter(regex='Pharmacy_Pharmacy').columns))\n",
    "print('Product (one-hot encoding):', len(dataframe.filter(regex='Product_').columns))\n",
    "print('Brand (one-hot encoding):', len(dataframe.filter(regex='Brand_').columns))\n",
    "print('Strength (one-hot encoding):', len(dataframe.filter(regex='Strength_').columns))\n",
    "print('\\nExternal features:')\n",
    "print('Weather:', len(dataframe.filter(regex='Precip|Humidity|WindSpeed|CloudCover|UVIndex|Temp').columns))\n",
    "print('Trends:', len(dataframe.filter(regex='Disease|Pain|Malaria|Influenza').columns)-2)  # remove MalariaCases and MalariaDeaths from this group\n",
    "print('Health/Holiday:', len(dataframe.filter(regex='MalariaCases|MalariaDeaths|AirPollution|Holiday').columns))\n",
    "print('\\nTotal:', len(dataframe.loc[:, 'Quantity_individual_1':'Strength_s'].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Create CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "d261c3f6-5b46-4817-b5f4-4113c3c3f074",
    "_uuid": "61c18067-4289-41ec-b864-164d0aeb67dd"
   },
   "outputs": [],
   "source": [
    "dataframe.to_csv('Malaria.csv')\n",
    "dataframeConstant.to_csv('MalariaConstant.csv')\n",
    "dataframeLumpy.to_csv('MalariaLumpy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
